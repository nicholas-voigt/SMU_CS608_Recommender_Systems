{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f2b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6393b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prediction  ground_truth\n",
      "A        0.81          0.88\n",
      "B        0.75          0.77\n",
      "C        0.50          0.07\n",
      "D        0.77          0.54\n",
      "E        0.59          0.39\n",
      "F        0.72          0.76\n",
      "G        0.49          0.44\n",
      "H        0.96          0.52\n",
      "I        0.84          0.45\n",
      "J        0.91          0.82\n"
     ]
    }
   ],
   "source": [
    "eval = np.array([\n",
    "    [0.81, 0.88],\n",
    "    [0.75, 0.77],\n",
    "    [0.50, 0.07],\n",
    "    [0.77, 0.54],\n",
    "    [0.59, 0.39],\n",
    "    [0.72, 0.76],\n",
    "    [0.49, 0.44],\n",
    "    [0.96, 0.52],\n",
    "    [0.84, 0.45],\n",
    "    [0.91, 0.82]\n",
    "\n",
    "])\n",
    "\n",
    "eval = pd.DataFrame(eval, columns=['prediction', 'ground_truth'], index=list('ABCDEFGHIJ'))\n",
    "\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de134e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2531797780234432\n"
     ]
    }
   ],
   "source": [
    "# RMSE calculation\n",
    "def rmse(predictions, targets):\n",
    "    return ((predictions - targets) ** 2).mean() ** 0.5\n",
    "\n",
    "print('RMSE:', rmse(eval['prediction'], eval['ground_truth']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce4df267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.196\n"
     ]
    }
   ],
   "source": [
    "# MAE calculation\n",
    "def mae(predictions, targets):\n",
    "    return (abs(predictions - targets)).mean()\n",
    "\n",
    "print('MAE:', mae(eval['prediction'], eval['ground_truth']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e29f7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prediction  ground_truth  pred_rank  gt_rank\n",
      "A        0.81          0.88        4.0      1.0\n",
      "B        0.75          0.77        6.0      3.0\n",
      "C        0.50          0.07        9.0     10.0\n",
      "D        0.77          0.54        5.0      5.0\n",
      "E        0.59          0.39        8.0      9.0\n",
      "F        0.72          0.76        7.0      4.0\n",
      "G        0.49          0.44       10.0      8.0\n",
      "H        0.96          0.52        1.0      6.0\n",
      "I        0.84          0.45        3.0      7.0\n",
      "J        0.91          0.82        2.0      2.0\n"
     ]
    }
   ],
   "source": [
    "# add two new columns with order according to previous columns\n",
    "eval['pred_rank'] = eval['prediction'].rank(ascending=False)\n",
    "eval['gt_rank'] = eval['ground_truth'].rank(ascending=False)\n",
    "\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c658db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.5515151515151515\n"
     ]
    }
   ],
   "source": [
    "# Pearson correlation\n",
    "def pearson_correlation(predictions, targets):\n",
    "    return np.sum((predictions - predictions.mean()) * (targets - targets.mean())) / np.sqrt(\n",
    "        ((predictions - predictions.mean()) ** 2).sum() * ((targets - targets.mean()) ** 2).sum())\n",
    "\n",
    "print('Pearson correlation:', pearson_correlation(eval['pred_rank'], eval['gt_rank']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c934fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.5515151515151515\n"
     ]
    }
   ],
   "source": [
    "# Spearman correlation\n",
    "def spearman_correlation(predictions, targets):\n",
    "    n = len(predictions)\n",
    "    return 1 - (6 * np.sum((predictions - targets) ** 2)) / (n * (n ** 2 - 1))\n",
    "\n",
    "print('Spearman correlation:', spearman_correlation(eval['pred_rank'], eval['gt_rank']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d23984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall rank correlation: 0.37777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Kendall rank correlation\n",
    "def kendall_rank_correlation(predictions, targets):\n",
    "    m = len(predictions) * (len(predictions) - 1) / 2\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(i + 1, len(predictions)):\n",
    "            if predictions.iloc[i] > predictions.iloc[j] and targets.iloc[i] > targets.iloc[j]:\n",
    "                correct += 1\n",
    "            elif predictions.iloc[i] < predictions.iloc[j] and targets.iloc[i] < targets.iloc[j]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "    return (correct - wrong) / m\n",
    "\n",
    "print('Kendall rank correlation:', kendall_rank_correlation(eval['pred_rank'], eval['gt_rank']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a7f03bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prediction  ground_truth  pred_rank  gt_rank  gt_observed\n",
      "A        0.81          0.88        4.0      1.0         True\n",
      "B        0.75          0.77        6.0      3.0         True\n",
      "C        0.50          0.07        9.0     10.0        False\n",
      "D        0.77          0.54        5.0      5.0         True\n",
      "E        0.59          0.39        8.0      9.0        False\n",
      "F        0.72          0.76        7.0      4.0         True\n",
      "G        0.49          0.44       10.0      8.0        False\n",
      "H        0.96          0.52        1.0      6.0         True\n",
      "I        0.84          0.45        3.0      7.0        False\n",
      "J        0.91          0.82        2.0      2.0         True\n"
     ]
    }
   ],
   "source": [
    "# Add column observed in ground truth, True if ground_truth > 0.5 else False\n",
    "eval['gt_observed'] = eval['ground_truth'] > 0.5\n",
    "\n",
    "print(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e92fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# AUC\n",
    "def auc(observation: pd.Series, prediction: pd.Series):\n",
    "    n = len(prediction)\n",
    "    pos = np.sum(observation)\n",
    "    total = pos * (n - pos)\n",
    "    correct_pairs = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if prediction.iloc[i] > prediction.iloc[j] and observation.iloc[i] == 1 and observation.iloc[j] == 0:\n",
    "                correct_pairs += 1\n",
    "            elif prediction.iloc[i] < prediction.iloc[j] and observation.iloc[i] == 0 and observation.iloc[j] == 1:\n",
    "                correct_pairs += 1\n",
    "    return correct_pairs / total\n",
    "\n",
    "print('AUC:', auc(eval['gt_observed'], eval['prediction']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c6220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8\n",
      "Recall: 0.67\n",
      "F1: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Precision, Recall, F1\n",
    "def precision_recall_f1(observation: pd.Series, prediction: pd.Series, k: int):\n",
    "    observations = np.sum(observation)\n",
    "    count = 0\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction.iloc[i] <= k and observation.iloc[i] == 1:\n",
    "            count += 1\n",
    "    precision = np.round(count / k, 2)\n",
    "    recall = np.round(count / observations, 2)\n",
    "    f1 = np.round(2 * (precision * recall) / (precision + recall), 2)\n",
    "    return precision, recall, f1\n",
    "\n",
    "precision, recall, f1 = precision_recall_f1(eval['gt_observed'], eval['pred_rank'], 5)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b225ac54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision: 0.8734126984126983\n"
     ]
    }
   ],
   "source": [
    "# Mean Average Precision (MAP)\n",
    "def mean_average_precision(observation: pd.Series, prediction: pd.Series):\n",
    "    df = pd.DataFrame({'observation': observation, 'prediction': prediction})\n",
    "    df.sort_values(by='prediction', ascending=True, inplace=True)\n",
    "    map_score = 0\n",
    "    obs = 0\n",
    "    for element in df.itertuples():\n",
    "        if element.observation == 1:\n",
    "            map_score += (obs + 1) / element.prediction\n",
    "            obs += 1\n",
    "    return map_score / obs\n",
    "\n",
    "print('Mean Average Precision:', mean_average_precision(eval['gt_observed'], eval['pred_rank']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5547dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e0127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Cumulative Reciprocal Rank: 0.9222546161321672\n"
     ]
    }
   ],
   "source": [
    "# Normalized Cumulative Reciprocal Rank (NCRR)\n",
    "def ncrr(observation: pd.Series, prediction: pd.Series):\n",
    "    df = pd.DataFrame({'observation': observation, 'prediction': prediction})\n",
    "    df.sort_values(by='prediction', ascending=True, inplace=True)\n",
    "    total_obs = np.sum(observation)\n",
    "    score = np.sum(np.fromiter([i.observation / i.prediction for i in df.itertuples()], dtype=float))\n",
    "    ideal_score = np.sum(1 / np.arange(1, total_obs + 1))\n",
    "    return score / ideal_score\n",
    "\n",
    "print('Normalized Cumulative Reciprocal Rank:', ncrr(eval['gt_observed'], eval['pred_rank']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d9250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discounted Cumulative Gain: 2.4484591188793923\n"
     ]
    }
   ],
   "source": [
    "# Discounted Cumulative Gain (DCG)\n",
    "def dcg(observation: pd.Series, prediction: pd.Series, k: int):\n",
    "    df = pd.DataFrame({'observation': observation, 'prediction': prediction})\n",
    "    df.sort_values(by='prediction', ascending=True, inplace=True)\n",
    "    score = np.sum(np.fromiter([df.observation.iloc[i] / np.log2(df.prediction.iloc[i] + 1) for i in range(k)], dtype=float))\n",
    "    return score\n",
    "\n",
    "print('Discounted Cumulative Gain:', dcg(eval['gt_observed'], eval['pred_rank'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d8fad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Discounted Cumulative Gain: 0.830419897363192\n"
     ]
    }
   ],
   "source": [
    "# Normalized Discounted Cumulative Gain (NDCG)\n",
    "def ndcg(observation: pd.Series, prediction: pd.Series, k: int):\n",
    "    df = pd.DataFrame({'observation': observation, 'prediction': prediction})\n",
    "    df.sort_values(by='prediction', ascending=True, inplace=True)\n",
    "    score = np.sum(np.fromiter([df.observation.iloc[i] / np.log2(df.prediction.iloc[i] + 1) for i in range(k)], dtype=float))\n",
    "    ideal_score = np.sum(np.fromiter([1 / np.log2(i + 1) for i in range(1, k + 1)], dtype=float))\n",
    "    return score / ideal_score\n",
    "\n",
    "print('Normalized Discounted Cumulative Gain:', ndcg(eval['gt_observed'], eval['pred_rank'], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e3d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs608-recsys-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
